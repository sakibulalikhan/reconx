#!/bin/bash

# Print script header
cat << "EOF"

___  __ \_____ _____________ _______ __  |/ /
__  /_/ /_  _ \_  ___/_  __ \__  __ \__    / 
_  _, _/ /  __// /__  / /_/ /_  / / /_    |  
/_/ |_|  \___/ \___/  \____/ /_/ /_/ /_/|_|                                             
+------------------------------------------------+
|         Author : @sakibulalikhan               |
+------------------------------------------------+

EOF

# Check if a URL is provided as an argument
if [ -z "$1" ]; then
    echo "Usage: $0 <target_url>"
    exit 1
fi

target_url="$1"
output_dir="$target_url/recon"

# Create necessary directories if they don't exist
mkdir -p "$output_dir/gowitness" "$output_dir/mantra" "$output_dir/gau" \
         "$output_dir/subdomains" "$output_dir/scans" "$output_dir/httprobe" \
         "$output_dir/potential_takeovers" "$output_dir/technologys" \
         "$output_dir/wayback/params" "$output_dir/wayback/extensions" "$output_dir/waf"

# Function to check if a tool is installed
check_tool() {
    if ! command -v "$1" >/dev/null 2>&1; then
        echo >&2 "Error: $1 is not installed. Please install it before running this script."
        exit 1
    fi
}

# Check for required tools
tools=("assetfinder" "amass" "subfinder" "jq" "httprobe" "subjack" "nmap" "naabu" "waybackurls" "gowitness" "mantra" "whatweb" "wafw00f")
for tool in "${tools[@]}"; do
    check_tool "$tool"
done

# Perform reconnaissance
echo
echo "#############################################"
echo "########### Start - Reconnaissance ##########"
echo "#############################################"
echo

# Harvesting subdomains with assetfinder
echo "[+] Harvesting subdomains with assetfinder..."
assetfinder "$target_url" >> "$output_dir/subdomains/final.txt"

# Double-checking for subdomains with amass
echo "[+] Double-checking for subdomains with amass..."
amass enum -d "$target_url" >> "$output_dir/subdomains/final.txt"

# Harvesting subdomains with subfinder
echo "[+] Harvesting subdomains with subfinder..."
subfinder -d "$target_url" >> "$output_dir/subdomains/final.txt"

# Harvesting subdomains using SSL cert with crt.sh
echo "[+] Harvesting subdomains using SSL cert with crt.sh..."
curl -s "https://crt.sh/?q=%.$target_url&output=json" | jq -r '.[].name_value' | sed 's/\*\.//g' | sort -u >> "$output_dir/subdomains/final.txt"

# Probing for alive domains
echo "[+] Probing for alive domains..."
cat "$output_dir/subdomains/final.txt" | sort -u | httprobe -s -p http:81 -p https:443 | sed 's/https\?:\/\///' | tr -d ':443' > "$output_dir/httprobe/alive.txt"

# Checking for possible subdomain takeover
echo "[+] Checking for possible subdomain takeover..."
subjack -w "$output_dir/subdomains/final.txt" -t 100 -timeout 30 -ssl -o "$output_dir/potential_takeovers/potential_takeovers.txt"

# Scanning for technologys
echo "[+] Scanning for technologys...."
whatweb -i "$output_dir/subdomains/final.txt" -a 3 -q --colour always --no-errors --log-verbose "$output_dir/technologys/tech_detected.txt"

# Scanning for open ports with Naabu
echo "[+] Scanning for open ports with Naabu..."
naabu -host "$output_dir/httprobe/alive.txt" -verify -rate 9000 -retries 1 -p 0-65535 -oA "$output_dir/scans/naabu-full.txt"

# Scraping JS files with Gau
echo "[+] Scraping JS files with Gau..."
cat "$output_dir/subdomains/final.txt" | gau | grep -iE '\.js' | grep -ivE '\.json' | sort -u >> "$output_dir/gau/gauJS.txt"

# Scraping API key or cred with Mantra
echo "[+] Scraping API key or cred with Mantra..."
cat "$output_dir/gau/gauJS.txt" | mantra >> "$output_dir/mantra/api_cred.txt"

# Scraping wayback data
echo "[+] Scraping wayback data..."
cat "$output_dir/subdomains/final.txt" | waybackurls >> "$output_dir/wayback/wayback_output.txt"
sort -u "$output_dir/wayback/wayback_output.txt"

# Pulling and compiling all possible parameters found in wayback data
echo "[+] Pulling and compiling all possible parameters found in wayback data..."
cat "$output_dir/wayback/wayback_output.txt" | grep '?*=' | cut -d '=' -f 1 | sort -u >> "$output_dir/wayback/params/wayback_params.txt"

# Pulling and compiling js/php/aspx/jsp/json files from wayback output
echo "[+] Pulling and compiling js/php/aspx/jsp/json files from wayback output..."
while IFS= read -r line; do
    ext="${line##*.}"
    case "$ext" in
        js|html|json|php|aspx)
            echo "$line" >> "$output_dir/wayback/extensions/$ext.txt"
            ;;
    esac
done < "$output_dir/wayback/wayback_output.txt"

# Capturing screenshots with gowitness
echo "[+] Capturing screenshots with gowitness..."
gowitness file "$output_dir/subdomains/final.txt" -d "$output_dir/gowitness/screenshots" -t 50

# Fingerprinting The Web Application Firewall
echo "[+] Fingerprinting The Web Application Firewall..."
wafw00f -i "$output_dir/subdomains/final.txt" -v | grep "The site" > "$output_dir/waf/detected_waf.txt"

echo
echo "#############################################"
echo "############## Final - Results ##############"
echo "#############################################"
echo

echo "[+] Harvesting Subdomain results are saved in: $output_dir/subdomains/final.txt"

echo "[+] Checking for possible subdomain takeovers are saved in: $output_dir/potential_takeovers/potential_takeovers.txt"

echo "[+] Detected technology information saved in: $output_dir/technologys/tech_detected.txt"

echo "[+] All open ports information saved in: $output_dir/scans/naabu-full.txt"

echo "[+] Scraped JS file links saved in: $output_dir/gau/gauJS.txt"

echo "[+] Scraped API key or cred saved in: $output_dir/mantra/api_cred.txt"

echo "[+] Wayback data saved in: $output_dir/wayback/wayback_output.txt"

echo "[+] All possible parameters found in wayback data saved in: $output_dir/wayback/params/wayback_params.txt"

echo "[+] All js/php/aspx/jsp/json files from wayback output saved in: $output_dir/wayback/extensions"

echo "[+] Screenshots captured by gowitness are saved in: $output_dir/gowitness/screenshots"

echo "[+] Script completed. & Thanks for using ReconX @sakibulalikhan."